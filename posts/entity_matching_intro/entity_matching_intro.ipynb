{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Intro to Entity Matching\"\n",
    "description: \"Matching entities in a sea of data\"\n",
    "image: \"thumbnail.png\"\n",
    "categories: [entity-matching, nlp, code, python, lm, gpt4, genre, rf]\n",
    "author: \"Santiago Velez\"\n",
    "date: \"7/21/2024\"\n",
    "date-modified: \"7/21/2024\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity matching is the process of identifying entity descriptions from different sources that refer to the same real-world entity. This post is an applied introduction to some common approaches used for entity matching. Since even for small datasets the number of possible matches can become excessive, the process is usually split in two:\n",
    "\n",
    "* **Blocking**: Generates the possible combinations and removes most of them based on simple metrics (ex: number of shared letters)\n",
    "* **Disambiguation**: Evaluates the likelihood that two entity descriptions refer to the same real-world entity. For Entity Disambiguation, the approaches can be split into the following categories:\n",
    "\n",
    "  * Prompting trained Large Language Models\n",
    "  * Bert-like Language Models Fine Tuned on Entity Tasks\n",
    "  * Similarity Metrics and traditional ML Models\n",
    "  * Rule systems developed by human experts\n",
    "\n",
    "There are multiple tasks related to Entity Matching. Though they are different, some approaches may solve common issues.\n",
    "\n",
    "* **Entity Disambiguation**: Evaluates the likelihood that two entity descriptions refer to the same real-world entity. Does not include the candidate generation and blocking process.\n",
    "* **Entity Linking**: Identifies characters in a text that match an entity in an existing Knowledge Base.\n",
    "* **Named Entity Recognition**: Identifies characters in a text that could be identified as Named Entities. Some of them also use Knowledge Bases or Disambiguation.\n",
    "\n",
    "**TL;DR**: This paragraph in \"Entity Matching using Large Language Models\" is a good summary of the current status of Entity Matching in 2024:\n",
    "   \n",
    "*\"We can summarize the high-level implications of our findings concerning the selection of matching techniques in the following rules of thumb: For use cases that do not involve many unseen entities and for which a decent amount of training data is available, PLM-based matchers are a suitable option which does not require much compute due to the smaller size of the models. For use cases that involve a relevant amount of unseen entities and for which it is costly to gather and maintain a decent size training set, LLM-based matchers should be preferred due to their high zero-shot performance and ability to generalize to unseen entities. If using the best performing hosted LLMs is not an option due to their high usage costs, fine-tuning a cheaper hosted model is an alternative that can deliver a similar F1 performance. If using using hosted models is no option due to privacy concerns, using an open-source LLM on local hardware can be an alternative providing a slightly lower F1 performance given that some task-specific training data or domain-specific matching rules are available.\"*\n",
    "\n",
    "*Ralph Peeters & Christian Bizer, [\"Entity Matching using Large Language Models\"](https://arxiv.org/pdf/2310.11244),  2024*\n",
    "\n",
    "A more detailed explanation of Entity Matching can be found [here](http://anhaidgroup.github.io/py_entitymatching/v0.4.0/user_manual/overview.html)\n",
    "\n",
    "## When to use AI for Entity Matching\n",
    "\n",
    "* If a human can find a match with the presented data, is likely that a Modern AI approach can do it too\n",
    "* If a human is having issues finding a match, AI won't be likely to help\n",
    "* If there's not enough information presented to the model, AI won't be able to find the match, even if a human has enough internal information to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following code on Google Colab clicking here.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/santiagomvc/santiagomvc.github.io/blob/main/posts/entity_matching_intro/entity_matching_intro.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading required libraries and packages\n",
    "!pip install transformers spacy numpy scipy py_entitymatching python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading spacy english package\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data for training/few-shot and evaluation\n",
    "synt_data = [\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"John A. Smith\", \"entity_2\": \"John A. Smith\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Emily J. Clarke\", \"entity_2\": \"Emily T. Clarke\", \"match\": \"no\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Sarah M. Johnson\", \"entity_2\": \"Sarah Marie Johnson\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"James P. Miller\", \"entity_2\": \"James P. Miles\", \"match\": \"no\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Michael O'Leary\", \"entity_2\": \"Michael OLeary\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Nancy L. Wright\", \"entity_2\": \"Nancy W. Wright\", \"match\": \"no\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Catherine G. Davis\", \"entity_2\": \"Catherine Grace Davis\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Richard A. Lee\", \"entity_2\": \"Richard A. Lin\", \"match\": \"no\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Robert K. Brown\", \"entity_2\": \"Robert K. Brown Jr.\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Karen M. Harris\", \"entity_2\": \"Karen M. Harrison\", \"match\": \"no\"},\n",
    "]\n",
    "\n",
    "real_data = [\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Spann-Wilder, Tiffany\", \"entity_2\": \"Tiffany Spann-Wilder\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Landon C. Dais\", \"entity_2\": \"Landon Dais\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Giglio JA\", \"entity_2\": \"Jodi Giglio\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"Brown, M\", \"entity_2\": \"Marla Gallo Brown\", \"match\": \"yes\"},\n",
    "    {\"chamber_1\": \"house\", \"chamber_2\": \"house\", \"entity_1\": \"J.T. 'Jabo' Waggoner\t\", \"entity_2\": \"Jabo Waggoner\", \"match\": \"yes\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Currently the best performing approach to Entity Disambiguation based on [recent research](https://arxiv.org/pdf/2310.11244). It usually consists of building a set of examples and assembling a prompt, in order to pass an unseen pair to the Language Model and get the relevant response. There's some exploration into using LM to enhance existing Entity Linking models.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Easy to use: create examples, compose a prompt, call an API, and parse the results.\n",
    "* Performance above older models trained specifically for entity matching\n",
    "* May not require structuring the data as much as other options, just collapsing all the information in a single string.\n",
    "* LM approach can be used to generate explanations and categories. Categories can be used to find common error cases (year, mixed words, etc.)\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Prompting is very sensible to changes\n",
    "* Most powerful models are paid or hard to use\n",
    "* No candidate generation and blocking (can be done with other tools)\n",
    "\n",
    "### Best LM Configuration - based on [*Entity Matching using LMs*]((https://arxiv.org/pdf/2310.11244)):\n",
    "* Model: GPT4\n",
    "* Prompt: Domain specific, complex prompt, free-form response (use regex to find 'yes')\n",
    "* Few Shot: Yes, related examples\n",
    "* Fine-tuning helps, especially smaller models, without losing generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading env and openai client\n",
    "load_dotenv(\".env\")   # Create an ENV file with OPENAI_API_KEY\n",
    "client = OpenAI()\n",
    "\n",
    "# Building prompts for few shot learning\n",
    "task_description = \"Do the two legislator names refer to the same real-world legislator?\"\n",
    "demostration = \"legislator_1: '{entity_1}'\\nlegislator_2: '{entity_2}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating examples for few shot learning\n",
    "few_shot_messages = []\n",
    "# System Prompt\n",
    "few_shot_messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "# Few shot Examples\n",
    "for example in synt_data:\n",
    "    # Adds description\n",
    "    few_shot_messages.append({\"role\": \"user\", \"content\": task_description})\n",
    "    # Adds example\n",
    "    few_shot_messages.append({\"role\": \"user\", \"content\": demostration.format(entity_1=example[\"entity_1\"], entity_2=example[\"entity_2\"])})\n",
    "    # Adds correct answer\n",
    "    few_shot_messages.append({\"role\": \"assistant\", \"content\": example[\"match\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1:  Spann-Wilder, Tiffany\n",
      "Entity 2:  Tiffany Spann-Wilder\n",
      "Same Entity:  yes\n",
      "Entity 1:  Landon C. Dais\n",
      "Entity 2:  Landon Dais\n",
      "Same Entity:  yes\n",
      "Entity 1:  Giglio JA\n",
      "Entity 2:  Jodi Giglio\n",
      "Same Entity:  No, the two names 'Giglio JA' and 'Jodi Giglio' do not seem to refer to the same real-world legislator. 'Giglio JA' is likely an abbreviation or a format that includes initials, and 'Jodi Giglio' is a full name. Without additional context, they are likely different individuals.\n",
      "Entity 1:  Brown, M\n",
      "Entity 2:  Marla Gallo Brown\n",
      "Same Entity:  no\n",
      "Entity 1:  J.T. 'Jabo' Waggoner\t\n",
      "Entity 2:  Jabo Waggoner\n",
      "Same Entity:  yes\n"
     ]
    }
   ],
   "source": [
    "# Running the model for each real example\n",
    "for sample in real_data:\n",
    "  messages = few_shot_messages.copy()\n",
    "  messages.append({\"role\": \"user\", \"content\": task_description})\n",
    "  messages.append({\"role\": \"user\", \"content\": demostration.format(entity_1=sample[\"entity_1\"], entity_2=sample[\"entity_2\"])})\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "  )\n",
    "  print(\"Entity 1: \", sample[\"entity_1\"])\n",
    "  print(\"Entity 2: \", sample[\"entity_2\"])\n",
    "  print(\"Same Entity: \", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-type Models Fine Tuned for Entity Tasks\n",
    "\n",
    "Older and smaller generation of Language Models, trained with general data but fine-tuned for entity tasks. Though some are trained for other tasks, most are trained for entity linking, which requires defining a Trie or Knowledge Base (usually WikiData). Changing the KB may require retraining the model\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* The Blocking process is performed automatically for the existing Trie/Knowledge Base\n",
    "* Good performance for data within training distribution\n",
    "* May not require additional training (depending on the model)\n",
    "* You can run it locally at no cost\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Hard to train from scratch, which is recommended if data is out of distribution\n",
    "* Limited Out of Distribution performance out of the box\n",
    "* Need to build a custom TRIE/KB or use generic KBs like WikiData\n",
    "\n",
    "### Recommended Models\n",
    "\n",
    "* [Facebook GENRE](https://github.com/facebookresearch/GENRE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Genre (Using Genre default Trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1:  Spann-Wilder, Tiffany\n",
      "Entity 1 KB Candidates:  ['Tiffany Spann-Wilder', 'Spann-Wilder, Tiffany', 'Tiffany Spann-Wilder, Tiffany']\n",
      "Entity 2:  Tiffany Spann-Wilder\n",
      "Entity 2 KB Candidates:  ['Tiffany Spann-Wilder', 'Tiffany Spann- Wilder', 'Tiffani Spann-Wilder']\n",
      "--\n",
      "Entity 1:  Landon C. Dais\n",
      "Entity 1 KB Candidates:  ['Landon C. Dais', 'Landon Dais', 'Landon C.Dais']\n",
      "Entity 2:  Landon Dais\n",
      "Entity 2 KB Candidates:  ['Landon Dais', 'LandonDais', 'Landon dais']\n",
      "--\n",
      "Entity 1:  Giglio JA\n",
      "Entity 1 KB Candidates:  ['Juan Antonio Giglio', 'Antonio Giglio', 'Jorge Antonio Giglio']\n",
      "Entity 2:  Jodi Giglio\n",
      "Entity 2 KB Candidates:  ['Jodi Giglio', 'Jodi Giglio (actress)', 'Jodi Giglio (singer)']\n",
      "--\n",
      "Entity 1:  Brown, M\n",
      "Entity 1 KB Candidates:  ['Mark Brown (American football)', 'Michael J. Brown', 'Michael Brown (American football)']\n",
      "Entity 2:  Marla Gallo Brown\n",
      "Entity 2 KB Candidates:  ['Marla Gallo Brown', 'Marla Gallo-Brown', 'Marla Gallo']\n",
      "--\n",
      "Entity 1:  J.T. 'Jabo' Waggoner\t\n",
      "Entity 1 KB Candidates:  ['J. T. Waggoner', \"J. T. 'Jabo' Waggoner\", 'Jabo Waggoner']\n",
      "Entity 2:  Jabo Waggoner\n",
      "Entity 2 KB Candidates:  ['Jabo Waggoner', 'Jabo Waggoner', 'Jambo Waggoner']\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Loading libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Loading model and tokenizers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-aidayago2\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-aidayago2\").eval()\n",
    "\n",
    "# Processing text\n",
    "sentences = []\n",
    "for example in real_data:\n",
    "    sentences.append(f\"[START_ENT] {example['entity_1']} [END_ENT]\")\n",
    "    sentences.append(f\"[START_ENT] {example['entity_2']} [END_ENT]\")\n",
    "\n",
    "# Running inference\n",
    "num_beams = 3\n",
    "outputs = model.generate(\n",
    "    **tokenizer(\n",
    "        sentences, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "    ),\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_beams,\n",
    "    # OPTIONAL: use constrained beam search\n",
    "    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n",
    ")\n",
    "preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Printing results\n",
    "i = 0\n",
    "for example in real_data:\n",
    "    print(\"Entity 1: \", example['entity_1'])\n",
    "    print(\"Entity 1 KB Candidates: \", preds[i:i+num_beams])\n",
    "    print(\"Entity 2: \", example['entity_2'])\n",
    "    print(\"Entity 2 KB Candidates: \", preds[i+num_beams:i+(num_beams*2)])\n",
    "    print(\"--\")\n",
    "    i += +(num_beams*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching systems with Small to No Training\n",
    "\n",
    "This group encompasses traditional Entity Matching techniques that use similarity metrics or classic ML models to evaluate how likely are the entities to match. Some of these approaches will require labeled examples to train models, though training is mostly managed by the library. This approach is popular for tabular datasets with multiple attributes\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Simpler matching algorithms (though libraries can be outdated)\n",
    "* Usually includes candidate generation and blocking\n",
    "* Fast\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Worse performance in Entity Disambiguation compared to LM-based approaches\n",
    "* Requires training data for some of the algorithms\n",
    "* Some libraries are old and have not been updated for some time\n",
    "\n",
    "### Matching Algorithms Examples\n",
    "\n",
    "* Jaccard similarity\n",
    "* Levenstein distance\n",
    "* Cosine similarity of vectors\n",
    "* Random Forest Similarity Classifier\n",
    "\n",
    "### Related libraries:\n",
    "  \n",
    "* [https://github.com/anhaidgroup/py_entitymatching](https://github.com/anhaidgroup/py_entitymatching)\n",
    "* [https://docs.dedupe.io/en/latest/](https://docs.dedupe.io/en/latest/)\n",
    "* [https://github.com/anhaidgroup/deepmatcher](https://github.com/anhaidgroup/deepmatcher)\n",
    "* [https://huggingface.co/shahrukhx01/paraphrase-mpnet-base-v2-fuzzy-matcher?text=fuzzformer](https://huggingface.co/shahrukhx01/paraphrase-mpnet-base-v2-fuzzy-matcher?text=fuzzformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### py_entitymatching + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import pandas as pd\n",
    "import py_entitymatching as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting data in required library configuration \n",
    "# Train set\n",
    "df = pd.DataFrame(synt_data)\n",
    "df = df.reset_index(names=\"id\").reset_index(names=\"ltable_id\").reset_index(names=\"rtable_id\")\n",
    "df[\"match\"] = df[\"match\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "A = df[['ltable_id', 'chamber_1', 'entity_1']].rename({'ltable_id': 'id', 'chamber_1': 'chamber', 'entity_1': 'entity'}, axis=1)\n",
    "B = df[['rtable_id', 'chamber_2', 'entity_2']].rename({'rtable_id': 'id', 'chamber_2': 'chamber', 'entity_2': 'entity'}, axis=1)\n",
    "C = df[[\"id\", \"ltable_id\", \"rtable_id\", \"match\"]]\n",
    "\n",
    "# Test set\n",
    "test_df = pd.DataFrame(real_data)\n",
    "test_df = test_df.reset_index(names=\"id\").reset_index(names=\"ltable_id\").reset_index(names=\"rtable_id\")\n",
    "test_df[\"match\"] = test_df[\"match\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "X = df[['ltable_id', 'chamber_1', 'entity_1']].rename({'ltable_id': 'id', 'chamber_1': 'chamber', 'entity_1': 'entity'}, axis=1)\n",
    "Y = df[['rtable_id', 'chamber_2', 'entity_2']].rename({'rtable_id': 'id', 'chamber_2': 'chamber', 'entity_2': 'entity'}, axis=1)\n",
    "Z = df[[\"id\", \"ltable_id\", \"rtable_id\", \"match\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registering tables metadata in the library\n",
    "em.set_key(A, 'id')\n",
    "em.set_key(B, 'id')\n",
    "em.set_key(C, 'id')\n",
    "em.set_ltable(C, B)\n",
    "em.set_ltable(C, A)\n",
    "\n",
    "em.set_key(X, 'id')\n",
    "em.set_key(Y, 'id')\n",
    "em.set_key(Z, 'id')\n",
    "em.set_ltable(Z, X)\n",
    "em.set_ltable(Z, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/py_entitymatching/blocker/overlap_blocker.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  l_df[l_dummy_overlap_attr] = l_df[l_overlap_attr]\n",
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/py_entitymatching/blocker/overlap_blocker.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r_df[r_dummy_overlap_attr] = r_df[r_overlap_attr]\n",
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/py_entitymatching/blocker/overlap_blocker.py:615: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[overlap_attr] = values\n"
     ]
    }
   ],
   "source": [
    "# Creating and blocking candidate matches (just for display)\n",
    "ob = em.OverlapBlocker()\n",
    "C = ob.block_tables(A, B, 'entity', 'entity', \n",
    "                    l_output_attrs=['entity', 'chamber'], \n",
    "                    r_output_attrs=['entity', 'chamber'],\n",
    "                    overlap_size=1, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround - Saving labeled data so we are able to load it in the library with the right metadata\n",
    "df = df.rename({'id':'_id', 'chamber_1': 'ltable_chamber', 'chamber_2': 'rtable_chamber', 'entity_1': 'ltable_entity',\n",
    "       'entity_2': 'rtable_entity', 'match': 'gold'}, axis=1)\n",
    "df.to_csv(\"temp_df.csv\")\n",
    "test_df = test_df.rename({'id':'_id', 'chamber_1': 'ltable_chamber', 'chamber_2': 'rtable_chamber', 'entity_1': 'ltable_entity',\n",
    "       'entity_2': 'rtable_entity', 'match': 'gold'}, axis=1)\n",
    "test_df.to_csv(\"temp_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# Load training data with metadata\n",
    "G = em.read_csv_metadata(\"temp_df.csv\", \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "# Load test data with metadata\n",
    "Z = em.read_csv_metadata(\"temp_test_df.csv\", \n",
    "                         key='_id',\n",
    "                         ltable=X, rtable=Y, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features for Entity Matching Model\n",
    "feature_table = em.get_features_for_matching(A, B, validate_inferred_attr_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Building feature vectors for entity matching model\n",
    "# Select the attrs. to be included in the feature vector table\n",
    "attrs_from_table = ['ltable_entity', 'ltable_chamber',\n",
    "                    'rtable_entity', 'rtable_chamber',]\n",
    "# Convert the labeled data to feature vectors using the feature table\n",
    "H = em.extract_feature_vecs(G, \n",
    "                            feature_table=feature_table, \n",
    "                            attrs_before = attrs_from_table,\n",
    "                            attrs_after='gold',\n",
    "                            show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training entity matching model with training data\n",
    "# Instantiate the RF Matcher\n",
    "rf = em.RFMatcher()\n",
    "# Get the attributes to be projected while training\n",
    "attrs_to_be_excluded = []\n",
    "attrs_to_be_excluded.extend(['_id', 'ltable_id', 'rtable_id', 'gold'])\n",
    "attrs_to_be_excluded.extend(attrs_from_table)\n",
    "# Train using feature vectors from the labeled data.\n",
    "rf.fit(table=H, exclude_attrs=attrs_to_be_excluded, target_attr='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/exp/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data for inference\n",
    "# Select the attrs. to be included in the feature vector table\n",
    "attrs_from_table = ['ltable_entity', 'ltable_chamber',\n",
    "                    'rtable_entity', 'rtable_chamber',]\n",
    "# Convert the cancidate set to feature vectors using the feature table\n",
    "L = em.extract_feature_vecs(Z, feature_table=feature_table,\n",
    "                             attrs_before= attrs_from_table,\n",
    "                             show_progress=False, n_jobs=-1)\n",
    "# Get the attributes to be excluded while predicting \n",
    "attrs_to_be_excluded = []\n",
    "attrs_to_be_excluded.extend(['_id', 'ltable_id', 'rtable_id'])\n",
    "attrs_to_be_excluded.extend(attrs_from_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>ltable_entity</th>\n",
       "      <th>ltable_chamber</th>\n",
       "      <th>rtable_entity</th>\n",
       "      <th>rtable_chamber</th>\n",
       "      <th>id_id_exm</th>\n",
       "      <th>id_id_anm</th>\n",
       "      <th>id_id_lev_dist</th>\n",
       "      <th>...</th>\n",
       "      <th>chamber_chamber_jac_qgm_3_qgm_3</th>\n",
       "      <th>entity_entity_jac_qgm_3_qgm_3</th>\n",
       "      <th>entity_entity_cos_dlm_dc0_dlm_dc0</th>\n",
       "      <th>entity_entity_jac_dlm_dc0_dlm_dc0</th>\n",
       "      <th>entity_entity_mel</th>\n",
       "      <th>entity_entity_lev_dist</th>\n",
       "      <th>entity_entity_lev_sim</th>\n",
       "      <th>entity_entity_nmw</th>\n",
       "      <th>entity_entity_sw</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spann-Wilder, Tiffany</td>\n",
       "      <td>house</td>\n",
       "      <td>Tiffany Spann-Wilder</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Landon C. Dais</td>\n",
       "      <td>house</td>\n",
       "      <td>Landon Dais</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Giglio JA</td>\n",
       "      <td>house</td>\n",
       "      <td>Jodi Giglio</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.945395</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Brown, M</td>\n",
       "      <td>house</td>\n",
       "      <td>Marla Gallo Brown</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.959048</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>J.T. 'Jabo' Waggoner\\t</td>\n",
       "      <td>house</td>\n",
       "      <td>Jabo Waggoner</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id  ltable_id  rtable_id           ltable_entity ltable_chamber  \\\n",
       "0    0          0          0   Spann-Wilder, Tiffany          house   \n",
       "1    1          1          1          Landon C. Dais          house   \n",
       "2    2          2          2               Giglio JA          house   \n",
       "3    3          3          3                Brown, M          house   \n",
       "4    4          4          4  J.T. 'Jabo' Waggoner\\t          house   \n",
       "\n",
       "          rtable_entity rtable_chamber  id_id_exm  id_id_anm  id_id_lev_dist  \\\n",
       "0  Tiffany Spann-Wilder          house          1        0.0             0.0   \n",
       "1           Landon Dais          house          1        1.0             0.0   \n",
       "2           Jodi Giglio          house          1        1.0             0.0   \n",
       "3     Marla Gallo Brown          house          1        1.0             0.0   \n",
       "4         Jabo Waggoner          house          1        1.0             0.0   \n",
       "\n",
       "   ...  chamber_chamber_jac_qgm_3_qgm_3  entity_entity_jac_qgm_3_qgm_3  \\\n",
       "0  ...                              1.0                       1.000000   \n",
       "1  ...                              1.0                       0.700000   \n",
       "2  ...                              1.0                       0.625000   \n",
       "3  ...                              1.0                       0.571429   \n",
       "4  ...                              1.0                       0.736842   \n",
       "\n",
       "   entity_entity_cos_dlm_dc0_dlm_dc0  entity_entity_jac_dlm_dc0_dlm_dc0  \\\n",
       "0                           1.000000                           1.000000   \n",
       "1                           0.666667                           0.500000   \n",
       "2                           0.666667                           0.500000   \n",
       "3                           0.666667                           0.500000   \n",
       "4                           0.500000                           0.333333   \n",
       "\n",
       "   entity_entity_mel  entity_entity_lev_dist  entity_entity_lev_sim  \\\n",
       "0           1.000000                     0.0               1.000000   \n",
       "1           0.973333                     1.0               0.933333   \n",
       "2           0.945395                     4.0               0.789474   \n",
       "3           0.959048                     2.0               0.866667   \n",
       "4           0.986667                     1.0               0.933333   \n",
       "\n",
       "   entity_entity_nmw  entity_entity_sw  predicted  \n",
       "0               13.0              13.0          1  \n",
       "1               14.0              14.0          0  \n",
       "2               12.0              12.0          1  \n",
       "3               12.0              12.0          0  \n",
       "4               13.0              13.0          1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the matches on inference data\n",
    "predictions = rf.predict(table=L, exclude_attrs=attrs_to_be_excluded,                          \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "### Python Libraries\n",
    "\n",
    "* [https://github.com/wbsg-uni-mannheim/MatchGPT/blob/main/LLMForEM](https://github.com/wbsg-uni-mannheim/MatchGPT/blob/main/LLMForEM)\n",
    "* [https://huggingface.co/facebook/mgenre-wiki](https://huggingface.co/facebook/mgenre-wiki)\n",
    "* [https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson](https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson)\n",
    "* [https://pypi.org/project/spacy-entity-linker/](https://pypi.org/project/spacy-entity-linker/)\n",
    "* [https://huggingface.co/shahrukhx01/paraphrase-mpnet-base-v2-fuzzy-matcher?text=fuzzformer](https://huggingface.co/shahrukhx01/paraphrase-mpnet-base-v2-fuzzy-matcher?text=fuzzformer)\n",
    "* [https://github.com/megagonlabs/ditto](https://github.com/megagonlabs/ditto)\n",
    "* [https://dedupe.io](https://dedupe.io)\n",
    "* [https://github.com/anhaidgroup/deepmatcher](https://github.com/anhaidgroup/deepmatcher)\n",
    "* [https://nbviewer.org/github/anhaidgroup/py_entitymatching](https://nbviewer.org/github/anhaidgroup/py_entitymatching/blob/master/notebooks/guides/end_to_end_em_guides/Basic%20EM%20Workflow%20Restaurants%20-%201.ipynb)\n",
    "* [https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)\n",
    "* [https://github.com/Babelscape/multinerd?tab=readme-ov-file](https://github.com/Babelscape/multinerd?tab=readme-ov-file)\n",
    "* [https://github.com/SapienzaNLP/extend?tab=readme-ov-file](https://github.com/SapienzaNLP/extend?tab=readme-ov-file)\n",
    "* [https://github.com/Lucaterre/spacyfishing](https://github.com/Lucaterre/spacyfishing)\n",
    "  \n",
    "### Papers\n",
    "\n",
    "* [LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking](https://arxiv.org/pdf/2407.04020)\n",
    "* [Entity Matching using Large Language Models](https://arxiv.org/pdf/2310.11244)\n",
    "* [EntGPT: Linking Generative Large Language Models with Knowledge Bases](https://arxiv.org/pdf/2402.06738)\n",
    "* [On Leveraging Large Language Models for Enhancing Entity Resolution](https://arxiv.org/pdf/2401.03426)\n",
    "* [Using ChatGPT for Entity Matching](https://arxiv.org/pdf/2305.03423)\n",
    "* [“Is This You?” Entity Matching in the Modern Data Stack with Large Language models](https://towardsdatascience.com/is-this-you-entity-matching-in-the-modern-data-stack-with-large-language-models-19a730373b26)\n",
    "* [DeepType: Multilingual Entity Linking by Neural Type System Evolution](https://arxiv.org/pdf/1802.01021)\n",
    "* [MultiNERD: A Multilingual, Multi-Genre and Fine-Grained Dataset for Named Entity Recognition (and Disambiguation)](https://aclanthology.org/2022.findings-naacl.60.pdf)\n",
    "* [AUTOREGRESSIVE ENTITY RETRIEVAL](https://arxiv.org/pdf/2010.00904)\n",
    "* [Deep Entity Matching with Pre-Trained Language Models](https://arxiv.org/pdf/2004.00584)\n",
    "* [https://github.com/sebastianruder/NLP-progress/blob/master/english/entity_linking.md](https://github.com/sebastianruder/NLP-progress/blob/master/english/entity_linking.md)\n",
    "* [https://openai.com/index/discovering-types-for-entity-disambiguation/](https://openai.com/index/discovering-types-for-entity-disambiguation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
