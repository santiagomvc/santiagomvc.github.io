[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog compiles technical notes from my experience working in Data Science and Machine Learning for the past few years."
  },
  {
    "objectID": "posts/installing_torch_cpu_with_poetry/installing_torch_cpu_with_poetry.html",
    "href": "posts/installing_torch_cpu_with_poetry/installing_torch_cpu_with_poetry.html",
    "title": "Installing Torch CPU with Poetry",
    "section": "",
    "text": "Having a working poetry environment that installs only cpu supported versions of torch is a good way to reduce the size of your docker container and speed up deployments. The following is a rough solution that seems to work (locally on Mac and Docker container) and could be used while torch and poetry solve their compatibility issues.\nInside your regular pyproject.toml file, include in [tool.poetry.dependencies] the following torch definition:\ntorch = [\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0%2Bcpu-cp39-cp39-linux_x86_64.whl\", markers=\"platform_system == \\\"Linux\\\"\"},\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0-cp39-none-macosx_10_9_x86_64.whl\", markers=\"platform_system == \\\"Darwin\\\" and platform_machine == \\\"x86_64\\\"\"},\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0-cp39-none-macosx_11_0_arm64.whl\", markers=\"platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\"}\n ]\nWhy is such an ugly solution required? Here are some apparent torch-poetry compatibility issues:\n\npoetry install torch==2.0.1 omits required gpu drivers for linux, which makes the container small but unusable Pytorch 2.0.1 pypi wheel does not install dependent cuda libraries pytorch/pytorch#100974.\npip and poetry install by default torch-cpu in mac and torch-gpu in linux . When specifying https://download.pytorch.org/whl/cpu as package source to install torch-cpu-linux, Poetry is unable to find a torch-cpu-mac version to use (Does not find a *+cpu version for mac). poetry add with –index-url option python-poetry/poetry#7685, Instructions for installing PyTorch python-poetry/poetry#6409 (comment).\npoetry may have issues dynamically selecting python wheels based on platforms (doesn’t happen if you use the wheel link) Install wheel based on platform python-poetry/poetry#1616.\n\nHere are some (so far) unsuccessful attempts to find a more elegant solution:\nAttempt 1:\n[tool.poetry.dependencies]\ntorch = { version = \"2.0.0\", source=\"torch\"}\n\n[[tool.poetry.source]]\nname = \"torch\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"explicit\" or \"suplemental\"\nAttempt 2:\n[tool.poetry.dependencies]\ntorch = [\n     {version = \"^2.0.0\", platform = \"darwin\"},\n     {version = \"^2.0.0\", platform = \"linux\", source = \"torch\"},\n     {version = \"^2.0.0\", platform = \"win32\", source = \"torch\"},\n]\n\n[[tool.poetry.source]]\nname = \"torch\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"explicit\"\nAttempt 3:\n[[tool.poetry.source]]\nname = \"torch_cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"supplemental\"\n\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[tool.poetry.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\", source=\"torch_cpu\" }\nAttempt 4:\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"linux_cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"supplemental\"\n\n[tool.poetry.group.linux_cpu]\noptional = true\n\n[tool.poetry.group.linux_cpu.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\", source=\"linux_cpu\"}\n\n[tool.poetry.group.darwin_cpu]\noptional = true\n\n[tool.poetry.group.darwin_cpu.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\"}\nIn most attempts, the error was around the inability to find a torch-cpu-mac version to install when the https://download.pytorch.org/whl/cpu repo was included."
  },
  {
    "objectID": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html",
    "href": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html",
    "title": "Applied Intro to Search Algorithms",
    "section": "",
    "text": "Information Retrieval and Reranking is a wide field with significant nuance and incredible importance to multiple businesses. This post is an applied introduction to building a simple search application, displaying three common approaches to search, one to reranking, and some basic evaluation metrics. This is not intended as a deep dive into each algorithm, but as a high-level display of the basic components required to build a search solution. For more information please review the references at the end of the post.\nTL;DR: BM25 is the default for a reason: simple, fast, and accurate. You should start there and use it as the benchmark for other approaches. If you have time and require increased accuracy, Colbert is worth exploring, but focus first on gathering data and building evaluation metrics.\nYou can run the following code on Google Colab clicking here."
  },
  {
    "objectID": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html#references",
    "href": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html#references",
    "title": "Applied Intro to Search Algorithms",
    "section": "References",
    "text": "References\n\nhttps://web.stanford.edu/class/cs276/handouts/lecture12-bm25etc.pdf\nhttps://zilliz.com/learn/mastering-bm25-a-deep-dive-into-the-algorithm-and-application-in-milvus\nhttps://huggingface.co/tasks/sentence-similarity\nhttps://github.com/stanford-futuredata/ColBERT\nhttps://arxiv.org/abs/2004.12832\nhttps://til.simonwillison.net/llms/colbert-ragatouille\nhttps://amenra.github.io/ranx/\nhttps://trec.nist.gov/pubs/trec2/papers/txt/23.txt"
  },
  {
    "objectID": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html",
    "href": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html",
    "title": "Is Artificial Intelligence a Risk for Public Policy?",
    "section": "",
    "text": "This is an extended version of the article published in Plural’s blog. You can find a shorter version here.\nCurrent Machine Learning models have the potential to automate and enhance activities in multiple fields, including Public Policy. Large Language Models can be used to summarize bills, extract entities, and even propose legislation. However, the same technology can be used to muffle the legislative process with biased responses, or help bad actors astroturf and hide harmful legislation. The complexity behind drafting and passing legislation makes the impact of AI even more difficult to understand."
  },
  {
    "objectID": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#policy-making-is-complex",
    "href": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#policy-making-is-complex",
    "title": "Is Artificial Intelligence a Risk for Public Policy?",
    "section": "Policy-Making is Complex",
    "text": "Policy-Making is Complex\nPolicy-making is a years-long process influenced by multiple actors, so the road from grassroots activism to executive signing can take significant time. The political nature of the work also makes it harder to get agreements even on fundamental levels. Bureaucracy combines with complexity to make the process inefficient and time-consuming, making most of the gains available to powerful organizations.\nTraditional data analysis and statistics have affected policy for a long time, from supporting data-based policies to predicting bill passing and election outcomes. However, the advent of powerful Machine Learning models and the advances in Natural Language Processing have allowed new and incredible use cases like identifying policy topics, extracting named entity jurisdictions, summarizing bills, and even chat-like question answering. This new technology has the ability to exert positive and negative influence on the policy-making process."
  },
  {
    "objectID": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#the-risks-of-using-ai-for-public-policy-work",
    "href": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#the-risks-of-using-ai-for-public-policy-work",
    "title": "Is Artificial Intelligence a Risk for Public Policy?",
    "section": "The Risks of Using AI for Public Policy Work",
    "text": "The Risks of Using AI for Public Policy Work\nMachine Learning and statistics can have a negative impact on Public Policy either by misunderstanding how models work or by using the models correctly for non-democratic objectives.\n\nBias in AI Data\nMachine Learning models learn from the data they are trained with, which means the input data has a defining effect on the model results. Since models are usually trained to minimize errors and maximize accuracy, it’s possible for models to learn artifacts that correlate with certain scenarios but are not causes of it. Models can also have great performance in average but regular performance for specific and important subgroups, which were likely less represented in the training data.\nResearch at MIT showed that around 2017 some commercial Gender Classification models displayed significant disparities in the classification of darker-skinned females and lighter-skinned males, with error rates of up to 34.7%. \nGiven the complexity, nuance, and variety in biases, there’s not a single and clear solution to the problems it raises. Google tried to improve its face recognition for its Pixel 4 phone, but it raised some criticism for the methods it used to gather the required data. This is a complex problem that must be treated accordingly.\n\n\nPersonal Information and Privacy Risks\nSince modern ML Models are being trained with terabytes and terabytes of data, it is almost impossible to manually verify that there’s no PII, private, or copyrighted information in the training sets. This means that, if the dataset was not cleaned correctly, your pictures, social security number, and medical information may be available without your permission. Even worse, some companies are consciously trying to collect this personal data with shady means.\nThere have been multiple cases in which police units have wrongfully jailed people due to errors in Face Recognition models, and more recently Samsung had to restrict access to ChatGPT due to leaked confidential information.\nAI providers are taking steps both to improve their data collection and the cases in which the technology can be used. OpenAI recently changed its policies to avoid collecting customers’ data for training by default, and multiple companies now restrict their Face Detection models to avoid military and police uses. At the end of the day though, companies will always want more data to train their models and more users to buy them, so this is a constant struggle. \n\n\nLack of Transparency in How Algorithms Work\nThe mathematical aspect of AI, ranging from linear algebra to information theory and density functions, is another source of opacity when integrating AI with public policy workers. This issue is amplified in Neural Networks, the building block to the best generative models, since there’s no simple interpretation of the parameters the model is learning, resulting in black box systems that experts find hard to trust. Adding to the transparency issues, the traditional openness around Machine Learning models and techniques is partially being replaced by shallow reports and closed releases, due in part to the strong competition in commercial AI.\nGPT4 Technical Report is an example of the current trend in AI development. While a few years back state-of-the-art models were shared publicly along with replication techniques, today most state-of-the-art models are kept private, and there’s not enough information shared to understand improvements and much less to try to replicate them. While some argue that this is part of Responsible AI principles to avoid misuse, others argue that it’s mostly about protecting private interests.\nGiven the tradition of open research in AI, there has been pushback around the new closed tendencies, like Facebook’s non-commercial release of their language models. And on the model opacity, there’s continued research around understanding how models work and impact different areas.\n\n\nModels are not perfect\nMost state-of-the-art AI models are probabilistic in nature, which means that they return the most likely answer, but there’s no strict enforcement on correctness, logic, or causality. Given the impressive results of Generative AI and the complexity behind it, it’s very easy for laypeople to misunderstand model results and use them in the wrong situations. To make matters worse, this complexity is often used by dishonest actors for their own personal gains.\nGiven the current capabilities of Language Models such as GPT4 and Bard, it’s easy to believe that they’re able to solve any task we ask. Memorizing large amounts of training data and being hyped by grifters does not help with the situation. However, multiple researchers have shown that the algorithms behind Language Models are unable to consistently solve multistep logical reasoning problems, for now. \nLanguage Models continue improving, and they may be able to evolve logical and analytical capabilities in the near future, whether by growing in size, improving world models, or other techniques we don’t even know yet. However, it’s important to understand and communicate the limitations of the current models we do have.\n\n\nNo Standards for AI Regulation\nThe growing complexity of Machine Learning and Generative algorithms, the quality jumps in model performance, the lack of technical knowledge in government, and the powerful actors involved are a brewing pot of issues for any attempt at regulation. \nAn example of the current regulatory status is the open discussion around AI Art. Tools like Stable Diffusion and Midjourney are incredible technologies that generate brand-new images in seconds, based on prompts made by users. These tools however are built using artists’ work and menace their monetizing abilities, since art with similar characteristics can be done in seconds. While artists have valid concerns, relying on copyright to address them may not be the best answer. \nIt seems like governments around the world are starting to understand Generative AI risks and opportunities, but there’s no clear path around regulation, and it’s important to note that some players may try to use it to stop competitors and maintain a competitive advantage. \n\n\nLack of Well-Defined Policy Goals\nThe Public Policy nature also contributes to the challenges around AI applications.The legal documents where public policy is supported are usually long, tangled, and full of legal terminology and technicalities. As any lawyer can confirm, words tend to have non-vernacular meanings and simple changes such as synonymous can significantly alter the legal meaning. Lastly, public policy is strongly connected to politics, where nuance and consensus are hard to find, and emotions are strong on either side of the aisle.   \nAn example of this is bills, which propose changes to current statutes. As such, what may seem like a simple change in a letter or a number can provoke meaningful transformations in public society, such as changing the minimum wage from $10 to $19. Arizona SCR 1023 is a good example of how small changes can significantly affect the meaning of the law.\nThere have been efforts to improve model performance in specific fields, as in Google’s latest release of med-palm 2, and there’s even open source work around specialized legal models. However, we need more targeted resources to work on the intersection of Public Policy and AI.\n\n\nUses cases that hurt democracy\nWhile most of the risks we described are provoked by misunderstandings or incompetence, there’s a very important issue left. Clever actors who understand AI and Public Policy can use modern technology in ways that undermine democracy. Those actions can be as simple as misleading descriptive statistics or as complex as deep-fake political images and simulated grassroots movements.\nResearchers at Stanford University displayed how GPT3 can be used to draft persuasive political messages. Similar models have been used to generate fake reviews on Amazon and to cheat in high school homework, and they could be used just as easily to fake public interest in relevant policy topics or to draft bills with undemocratic objectives. \nThere’s a growing understanding in policy circles that this technology can affect public policy, and senators have publically interacted with some of the tools. Even AI companies are limiting the ways the models can be used in policy settings, but it’s not enough. This technology exists and is being used right now, and the Plural, Public, and Private sectors need to agree on the ways AI should not be used to affect public policy."
  },
  {
    "objectID": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#the-biggest-challenge",
    "href": "posts/ai_risks_for_public_policy/ai_risks_for_public_policy.html#the-biggest-challenge",
    "title": "Is Artificial Intelligence a Risk for Public Policy?",
    "section": "The Biggest Challenge",
    "text": "The Biggest Challenge\nPublic Policy impacts everyone, and there’s a real risk in how slowly government and public institutions will adapt and how fast will actors move and break things. We can see how unregulated new technology can impact the lives of thousands of people, and how sooner action could have prevented some of the harm.\nRight now we need strong pressure from the public and grassroots organizations to accelerate government action and guarantee that corporate interests are aligned with the public benefit, like Google employees did with Project Maven, while maintaining incentives to continue research that improves the lives of the community.\nWhile this article focuses on risks around ML and Generative AI, it’s also clear that the technology can be used to make the public policy process more accessible and democratic, as we argue in Summarizing Bills With Generative AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMVC Notes",
    "section": "",
    "text": "Applied Intro to Search Algorithms\n\n\n\n\n\n\nsearch\n\n\nreranking\n\n\ncode\n\n\npython\n\n\nbm25\n\n\ncolbert\n\n\nsentence-transformers\n\n\n\nRetrieval and Reranking in Python\n\n\n\n\n\nJul 14, 2024\n\n\nSantiago Velez\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Torch CPU with Poetry\n\n\n\n\n\n\ncode\n\n\npython\n\n\ntorch\n\n\npoetry\n\n\n\nIt sucks, at least for now\n\n\n\n\n\nSep 11, 2023\n\n\nSantiago Velez\n\n\n\n\n\n\n\n\n\n\n\n\nIs Artificial Intelligence a Risk for Public Policy?\n\n\n\n\n\n\nai\n\n\npublic policy\n\n\n\nSomeone needs to talk about it\n\n\n\n\n\nMay 22, 2023\n\n\nSantiago Velez\n\n\n\n\n\n\nNo matching items"
  }
]