[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog compiles technical notes from my experience working in Data Science and Machine Learning for the past few years."
  },
  {
    "objectID": "posts/installing_torch_cpu_with_poetry/installing_torch_cpu_with_poetry.html",
    "href": "posts/installing_torch_cpu_with_poetry/installing_torch_cpu_with_poetry.html",
    "title": "Installing Torch CPU with Poetry",
    "section": "",
    "text": "Having a working poetry environment that installs only cpu supported versions of torch is a good way to reduce the size of your docker container and speed up deployments. The following is a rough solution that seems to work (locally on Mac and Docker container) and could be used while torch and poetry solve their compatibility issues.\nInside your regular pyproject.toml file, include in [tool.poetry.dependencies] the following torch definition:\ntorch = [\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0%2Bcpu-cp39-cp39-linux_x86_64.whl\", markers=\"platform_system == \\\"Linux\\\"\"},\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0-cp39-none-macosx_10_9_x86_64.whl\", markers=\"platform_system == \\\"Darwin\\\" and platform_machine == \\\"x86_64\\\"\"},\n     {url=\"https://download.pytorch.org/whl/cpu/torch-2.0.0-cp39-none-macosx_11_0_arm64.whl\", markers=\"platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\"}\n ]\nWhy is such an ugly solution required? Here are some apparent torch-poetry compatibility issues:\n\npoetry install torch==2.0.1 omits required gpu drivers for linux, which makes the container small but unusable Pytorch 2.0.1 pypi wheel does not install dependent cuda libraries pytorch/pytorch#100974.\npip and poetry install by default torch-cpu in mac and torch-gpu in linux . When specifying https://download.pytorch.org/whl/cpu as package source to install torch-cpu-linux, Poetry is unable to find a torch-cpu-mac version to use (Does not find a *+cpu version for mac). poetry add with –index-url option python-poetry/poetry#7685, Instructions for installing PyTorch python-poetry/poetry#6409 (comment).\npoetry may have issues dynamically selecting python wheels based on platforms (doesn’t happen if you use the wheel link) Install wheel based on platform python-poetry/poetry#1616.\n\nHere are some (so far) unsuccessful attempts to find a more elegant solution:\nAttempt 1:\n[tool.poetry.dependencies]\ntorch = { version = \"2.0.0\", source=\"torch\"}\n\n[[tool.poetry.source]]\nname = \"torch\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"explicit\" or \"suplemental\"\nAttempt 2:\n[tool.poetry.dependencies]\ntorch = [\n     {version = \"^2.0.0\", platform = \"darwin\"},\n     {version = \"^2.0.0\", platform = \"linux\", source = \"torch\"},\n     {version = \"^2.0.0\", platform = \"win32\", source = \"torch\"},\n]\n\n[[tool.poetry.source]]\nname = \"torch\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"explicit\"\nAttempt 3:\n[[tool.poetry.source]]\nname = \"torch_cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"supplemental\"\n\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[tool.poetry.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\", source=\"torch_cpu\" }\nAttempt 4:\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"linux_cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\npriority = \"supplemental\"\n\n[tool.poetry.group.linux_cpu]\noptional = true\n\n[tool.poetry.group.linux_cpu.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\", source=\"linux_cpu\"}\n\n[tool.poetry.group.darwin_cpu]\noptional = true\n\n[tool.poetry.group.darwin_cpu.dependencies]\ntorch = { version = \"&gt;=2.0.0, !=2.0.1\"}\nIn most attempts, the error was around the inability to find a torch-cpu-mac version to install when the https://download.pytorch.org/whl/cpu repo was included."
  },
  {
    "objectID": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html",
    "href": "posts/intro_to_search_algorithms/intro_to_search_algorithms.html",
    "title": "Applied Intro to Search Algorithms",
    "section": "",
    "text": "You can run the following code on Google Colab clicking here.\n  \n\nInstall required libraries\nTested with python 3.10.1\n\n!pip install numpy panda torch langchain-text-splitters sentence-transformers rank_bm25 faiss-cpu ranx ragatouille==0.0.8 llama-index==0.9.48\n\n\n\nImport libraries\n\nimport pandas as pd\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom ragatouille import RAGPretrainedModel\nfrom rank_bm25 import BM25Okapi\nfrom ranx import Qrels, Run, fuse, evaluate\n\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\nthe same time. Both libraries are known to be incompatible and this\ncan cause random crashes or deadlocks on Linux when loaded in the\nsame Python program.\nUsing threadpoolctl may cause crashes or deadlocks. For more\ninformation and possible workarounds, please see\n    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n\n  warnings.warn(msg, RuntimeWarning)\n\n\n\n\nDefine utility functions\nWe define a simple text processing function. Possible improvements include tokenization, stemming, stopword removal, etc.\n\ndef text_preprocess(text):\n    text = text.lower()\n    return text\n\n\ndef build_run(results_df, doc_id_col=\"chunk_id\"):\n    run_df = results_df.copy()\n    run = Run.from_df(\n        df=run_df,\n        q_id_col=\"query_id\",\n        doc_id_col=doc_id_col,\n        score_col=\"score\",\n    )\n    return run\n\n\n\nPrepare data\n\nLoad data\n\ntexts_df = pd.read_csv(\"https://raw.githubusercontent.com/santiagomvc/search_methods_intro/main/data/texts.csv\")\nqueries_df = pd.read_csv(\"https://raw.githubusercontent.com/santiagomvc/search_methods_intro/main/data/queries.csv\")\n\n\n\nChunking configuration\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=256,\n    chunk_overlap=64,\n    length_function=len,\n    is_separator_regex=False,\n)\n\n\n\nSplit data and process chunks\n\ndoc_ids = []\nchunk_ids = []\nchunk_texts = []\nfor _, row in texts_df.iterrows():\n    doc_id = str(row[\"doc_id\"])\n    doc_chunk_texts = text_splitter.split_text(row[\"doc_text\"])\n    n_chunk_texts = len(doc_chunk_texts)\n    doc_chunk_ids = [f\"{doc_id}-{str(i)}\" for i in range(n_chunk_texts)]\n    # basic text processing\n    doc_chunk_texts = [text_preprocess(chunk) for chunk in doc_chunk_texts]\n    # save results\n    doc_ids.extend([doc_id] * n_chunk_texts)\n    chunk_ids.extend(doc_chunk_ids)\n    chunk_texts.extend(doc_chunk_texts)\n\n\n\nSave results as df\n\nchunks_df = pd.DataFrame({\n    \"doc_id\": doc_ids,\n    \"chunk_id\": chunk_ids,\n    \"chunk_text\": chunk_texts,\n})\n\n\n\n\nIndexing data\n\nBM25\n\nbm_25_tokenized_corpus = [chunk.split(\" \") for chunk in chunk_texts]\nbm25_index = BM25Okapi(bm_25_tokenized_corpus)\n\n\n\nSentence Transformers + Faiss Index\n\nsentsim_model = SentenceTransformer(\"all-mpnet-base-v2\")\nsentsim_embeddings = sentsim_model.encode(chunk_texts)\nsentsim_embedding_size = sentsim_embeddings.shape[1]\nsentsim_index = faiss.IndexFlatL2(sentsim_embedding_size)\nsentsim_index.add(sentsim_embeddings)\n\n\n\nColbert + RAGatuille\n\nif __name__ == \"__main__\":   # Required so ragatouille runs safely\n    try:\n        colbert_index = RAGPretrainedModel.from_index(\".ragatouille/colbert/indexes/index\")\n    except:\n        colbert_index = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n        colbert_index.index(\n            index_name=\"index\", \n            collection=chunk_texts, \n            document_ids=chunk_ids, \n            use_faiss=False,\n            max_document_length=1024,\n            split_documents=False,\n        )\n\n[Jul 02, 18:29:22] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n\n\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n\n\n\n\n\nSearch Functions\n\nBM25\n\ndef bm25_search(query_text, bm25_index=bm25_index, chunks_df=chunks_df):\n    # Preprocess query same as docs\n    query_text = text_preprocess(query_text)\n    # Transform query\n    tokenized_query = query_text.split(\" \")\n    # Search with bm25 index\n    doc_scores = bm25_index.get_scores(tokenized_query)\n    # Format as dataframe\n    bm25_df = chunks_df.copy()\n    bm25_df[\"score\"] = doc_scores\n    bm25_df = bm25_df.loc[bm25_df[\"score\"] &gt; 0]\n    # Drop to get docs, no chunks\n    bm25_df = bm25_df.sort_values(\"score\", ascending=False)\n    bm25_df = bm25_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n    # Return results\n    return bm25_df\n\n\n\nSingle Vector Sentece Similarity\n\ndef sentsim_search(query_text, sentsim_model=sentsim_model, sentsim_index=sentsim_index, chunks_df=chunks_df, k=5):\n    # Preprocess query same as docs\n    query_text = text_preprocess(query_text)\n    # Encode query\n    sentsim_query_emb = sentsim_model.encode(query_text).reshape(1,-1)\n    # Search with embedding\n    D, I = sentsim_index.search(sentsim_query_emb, k)\n    # Format as dataframe\n    sentsim_df = chunks_df.copy()\n    sentsim_df = sentsim_df.loc[I[0]]\n    sentsim_df[\"score\"] = D[0].astype(float)\n    # Drop to get docs, no chunks\n    sentsim_df = sentsim_df.sort_values(\"score\", ascending=False)\n    sentsim_df = sentsim_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n    return sentsim_df\n\n\n\nColbert\n\ndef colbert_search(query_text, colbert_index=colbert_index, chunks_df=chunks_df, k=5):\n    # Preprocess query same as docs\n    query_text = text_preprocess(query_text)\n    # Run query\n    colbert_results = colbert_index.search(query_text, k=k)\n    # Save results as a df\n    colbert_df = pd.DataFrame(colbert_results)\n    colbert_df = colbert_df.rename({\"document_id\": \"chunk_id\"}, axis=1)\n    colbert_df = colbert_df.merge(chunks_df, how=\"left\", on=\"chunk_id\")\n    colbert_df = colbert_df[[\"doc_id\", \"chunk_id\", \"chunk_text\", \"score\"]]\n    # Drop to get docs, no chunks\n    colbert_df = colbert_df.sort_values(\"score\", ascending=False)\n    colbert_df = colbert_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n    return colbert_df\n\n\n\nRank Fusion (Min-Max Norm, CombMAX fusion)\n\ndef combined_search(query_text, fusion_norm=\"min-max\", fusion_method=\"max\", chunks_df=chunks_df):\n    runs = []\n    for search_fun in [bm25_search, sentsim_search, colbert_search]:\n        # Save results in Run format\n        run_df = search_fun(query_text)\n        run_df[\"query_id\"] = \"0\"   # query id is required for the run\n        # run_df[\"chunk_id\"] = run_df[\"chunk_id\"].astype(str)\n        run = build_run(run_df)\n        runs.append(run)\n    ## Combining runs\n    combined_run = fuse(\n        runs=runs,\n        norm=fusion_norm,\n        method=fusion_method,\n    )\n    ## Saving as dataframe\n    combined_df = combined_run.to_dataframe()\n    combined_df = combined_df.drop(\"q_id\", axis=1)\n    combined_df = combined_df.rename({\"doc_id\": \"chunk_id\"}, axis=1)\n    combined_df = combined_df.merge(chunks_df, how=\"left\", on=\"chunk_id\")\n    combined_df = combined_df[[\"doc_id\", \"chunk_id\", \"chunk_text\", \"score\"]]\n    # Drop to get docs, no chunks\n    combined_df = combined_df.sort_values(\"score\", ascending=False)\n    combined_df = combined_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n    ## Return similar format to other responses\n    return combined_df\n\n\n\nGlobal Search Function\n\ndef search(query_text, mode=\"bm25\"):\n    if mode==\"bm25\":\n        return bm25_search(query_text)\n    elif mode==\"sentsim\":\n        return sentsim_search(query_text)\n    elif mode==\"colbert\":\n        return colbert_search(query_text)\n    elif mode==\"combined\":\n        return combined_search(query_text)\n\n\n\n\nEvaluates search with labeled queries\n\nEvaluation function\n\ndef evaluate_search(mode=\"bm25\", queries_df=queries_df):\n    # Preprocess df\n    queries_df[\"query_id\"] = queries_df[\"query_id\"].astype(str)\n    queries_df[\"doc_id\"] = queries_df[\"doc_id\"].astype(str)\n    queries_df.loc[queries_df[\"score\"] &gt; 0, \"score\"] = 1   # Replace all positive scores with 1\n    # Create Qrel for evaluation\n    qrels = Qrels.from_df(\n        df=queries_df,\n        q_id_col=\"query_id\",\n        doc_id_col=\"doc_id\",\n        score_col=\"score\",\n    )\n\n    # Get search responses\n    unique_queries_df = queries_df[[\"query_id\", \"query_text\"]].drop_duplicates()\n    unique_queries_list = unique_queries_df.values.tolist()\n    responses_list = []\n    for query_id, query_text in unique_queries_list:\n        response_df = search(query_text, mode=mode)\n        response_df[\"query_id\"] = query_id\n        responses_list.append(response_df)\n\n    # Build run dataframe\n    run_df = pd.concat(responses_list)\n    run_df[\"doc_id\"] = run_df[\"doc_id\"].astype(str)\n    run = build_run(run_df, doc_id_col=\"doc_id\")\n\n    ## Evaluate run\n    metrics = evaluate(qrels, run, [\"f1\", \"mrr\"])\n    print(mode, metrics)\n\n\n\nEvaluate BM25\n\nevaluate_search(mode=\"bm25\")\n\nbm25 {'f1': 0.6981481481481481, 'mrr': 0.9166666666666666}\n\n\n\n\nEvaluate Single Vector Sentence Similarity\n\nevaluate_search(mode=\"sentsim\")\n\nsentsim {'f1': 0.6555555555555556, 'mrr': 0.8888888888888888}\n\n\n\n\nEvaluate Colbert\n\nevaluate_search(mode=\"colbert\")\n\nLoading searcher for index index for the first time... This may take a few seconds\n[Jul 02, 18:29:35] #&gt; Loading codec...\n[Jul 02, 18:29:35] #&gt; Loading IVF...\n[Jul 02, 18:29:35] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n[Jul 02, 18:29:36] #&gt; Loading doclens...\n[Jul 02, 18:29:36] #&gt; Loading codes and residuals...\n[Jul 02, 18:29:36] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n[Jul 02, 18:29:36] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\nSearcher loaded!\n\n#&gt; QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n#&gt; Input: . juneteenth,          True,       None\n#&gt; Output IDs: torch.Size([32]), tensor([  101,     1,  2238, 17389,  3372,  2232,   102,   103,   103,   103,\n          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n          103,   103])\n#&gt; Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\ncolbert {'f1': 0.7944444444444444, 'mrr': 1.0}\n\n\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n100%|████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 603.06it/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 65.29it/s]\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n\n\n\n\nEvaluate Rank Fusion (Min-Max Norm, CombMAX fusion)\n\nevaluate_search(mode=\"combined\")\n\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n\n\ncombined {'f1': 0.6814814814814815, 'mrr': 0.8888888888888888}\n\n\n\n\n\nTry it yourself!\n\nquery_text = input(\"Enter your query:\")\nsearch_mode = input(\"Enter the search mode (bm25, sentsim, colbert, combined):\")\nsearch(query_text, search_mode).to_dict(orient=\"records\")\n\nEnter your query: health\nEnter the search mode (bm25, sentsim, colbert, combined): combined\n\n\n/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n\n\n[{'doc_id': '16',\n  'chunk_id': '16-4',\n  'chunk_text': \"13 vacation rentals or short-term rentals as follows:\\n14 1. to protect the public's health and safety, including rules and\\n15 regulations related to fire and building codes, health and sanitation,\",\n  'score': 1.0},\n {'doc_id': '10',\n  'chunk_id': '10-48',\n  'chunk_text': '110 of social work, psychologist licensed by the board of psychology, or other licensed counseling\\n111 professional with appropriate experience and training, provided that any such individual makes progress',\n  'score': 1.0},\n {'doc_id': '17',\n  'chunk_id': '17-56',\n  'chunk_text': '41 (10) \"health care provider\" or \"provider\" means any person or entity li-\\n42 censed, certified, or otherwise authorized by law to administer health care\\n43 in the ordinary course of business or practice of a profession, including',\n  'score': 1.0},\n {'doc_id': '14',\n  'chunk_id': '14-76',\n  'chunk_text': '176 analysts, and other licensed health and behavioral positions, which may either be employed by the\\n177 school board or provided through contracted services.',\n  'score': 0.439275072516164},\n {'doc_id': '12',\n  'chunk_id': '12-42',\n  'chunk_text': '100 deduction for such taxable year for long-term health care insurance premiums paid by him.\\n101 11. contract payments to a producer of quota tobacco or a tobacco quota holder, or their spouses, as',\n  'score': 0.1682393388722695},\n {'doc_id': '13',\n  'chunk_id': '13-6',\n  'chunk_text': '19 conditions of employment of the workforce \\n20 \\n21 . however, no locality shall adopt any workplace rule, other than for the purposes of a\\n22 community services board or behavioral health authority as defined in § 37.2-100, that prevents an',\n  'score': 0.01917413339973471},\n {'doc_id': '5',\n  'chunk_id': '5-7',\n  'chunk_text': '18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 5. the mental and physical health of all individuals involved.\\n28 6. which parent is more likely to allow the child frequent,\\n29 meaningful and continuing contact with the other parent. this paragraph',\n  'score': 0.0}]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMVC Notes",
    "section": "",
    "text": "Applied Intro to Search Algorithms\n\n\n\n\n\n\nsearch\n\n\nreranking\n\n\ncode\n\n\npython\n\n\nbm25\n\n\ncolbert\n\n\nsentence-transformers\n\n\n\nRetrieval and Reranking in Python\n\n\n\n\n\nJul 14, 2024\n\n\nSantiago Velez\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Torch CPU with Poetry\n\n\n\n\n\n\ncode\n\n\npython\n\n\ntorch\n\n\npoetry\n\n\n\nIt sucks, at least for now\n\n\n\n\n\nSep 11, 2023\n\n\nSantiago Velez\n\n\n\n\n\n\nNo matching items"
  }
]